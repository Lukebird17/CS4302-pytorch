# CUDAç®—å­Profilingä½¿ç”¨æŒ‡å—ï¼ˆå¢å¼ºç‰ˆï¼‰

## ğŸ¯ ç›®æ ‡

ä½¿ç”¨PyTorch Profilerå…¨é¢åˆ†æTransformer/BERTæ¨¡å‹çš„CUDAç®—å­è°ƒç”¨æƒ…å†µï¼Œä¸ºå¤§ä½œä¸šçš„ç®—å­è°ƒç ”æä¾›æ•°æ®æ”¯æŒã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•1: ä½¿ç”¨è‡ªåŠ¨åŒ–è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
cd /home/honglianglu/hdd/CS4302-pytorch/lab1_workspace/03_profiling
./run_comprehensive_profiling.sh
```

### æ–¹æ³•2: æ‰‹åŠ¨è¿è¡ŒPythonè„šæœ¬

```bash
# å¿«é€Ÿæµ‹è¯•ï¼ˆå°è§„æ¨¡ï¼‰
python3 profile_bert.py \
    --batch-sizes 1 8 \
    --seq-lens 128 \
    --output-dir ./profiling_results/quick

# å®Œæ•´æµ‹è¯•ï¼ˆå¤šç§é…ç½®ï¼‰
python3 profile_bert.py \
    --use-real-bert \
    --batch-sizes 1 4 8 16 \
    --seq-lens 128 256 512 \
    --output-dir ./profiling_results/full
```

## ğŸ“Š è¾“å‡ºæ–‡ä»¶è¯´æ˜

æ¯ä¸ªé…ç½®ä¼šç”Ÿæˆ3ä¸ªæ–‡ä»¶ï¼š

1. **`profiling_stats_bs{N}_seq{M}.json`** - è¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡æ•°æ®
   - Topç®—å­åˆ—è¡¨
   - ç®—å­åˆ†ç±»ç»Ÿè®¡
   - Native functionæ˜ å°„
   - CUDA kernelä¿¡æ¯

2. **`kernel_analysis_report_bs{N}_seq{M}.md`** - Markdownæ ¼å¼åˆ†ææŠ¥å‘Š
   - å®éªŒé…ç½®
   - æ€§èƒ½æ€»è§ˆ
   - Top 10ç®—å­è¯¦ç»†åˆ†æ
   - CUDAå®ç°æ–‡ä»¶è·¯å¾„
   - è°ƒç ”è¦ç‚¹å»ºè®®

3. **`bert_trace_bs{N}_seq{M}.json`** - Chrome Traceå¯è§†åŒ–æ–‡ä»¶
   - åœ¨Chromeæµè§ˆå™¨ä¸­æ‰“å¼€ `chrome://tracing`
   - åŠ è½½æ­¤æ–‡ä»¶æŸ¥çœ‹æ—¶é—´çº¿

## ğŸ“‹ æŸ¥çœ‹ç»“æœ

### 1. å¿«é€ŸæŸ¥çœ‹Topç®—å­

```bash
# æŸ¥çœ‹Top 3ç®—å­
python3 -c "
import json
with open('profiling_results/quick/profiling_stats_bs8_seq128.json') as f:
    data = json.load(f)
    for i, op in enumerate(data['top_aten_operators'][:3], 1):
        print(f'{i}. {op[\"name\"]}')
        print(f'   æ—¶é—´: {op[\"cuda_time_total_ms\"]:.2f}ms')
        print(f'   æ–‡ä»¶: {op[\"potential_cuda_file\"]}')
        print()
"
```

### 2. æŸ¥çœ‹åˆ†ææŠ¥å‘Š

```bash
ls profiling_results/*/kernel_analysis_report_*.md
cat profiling_results/quick/kernel_analysis_report_bs8_seq128.md
```

### 3. Chromeå¯è§†åŒ–

1. æ‰“å¼€Chromeæµè§ˆå™¨
2. è®¿é—® `chrome://tracing`
3. ç‚¹å‡» "Load" æŒ‰é’®
4. é€‰æ‹© `bert_trace_*.json` æ–‡ä»¶
5. ä½¿ç”¨WASDé”®å¯¼èˆªï¼Œé¼ æ ‡ç¼©æ”¾æŸ¥çœ‹

## ğŸ” å…³é”®ç®—å­è¯†åˆ«

è„šæœ¬ä¼šè‡ªåŠ¨è¯†åˆ«ä»¥ä¸‹ç±»åˆ«çš„ç®—å­ï¼š

- **Matrix Operations**: mm, matmul, bmm, addmm, gemm
- **Normalization**: layer_norm, batch_norm
- **Activation**: gelu, relu, softmax, sigmoid
- **Attention**: attention, scaled_dot_product
- **Embedding**: embedding, gather
- **Elementwise**: add, mul, div, sub
- **Memory**: copy, clone, transpose, view

## ğŸ“ å…¸å‹çš„Topç®—å­ï¼ˆå‚è€ƒï¼‰

æ ¹æ®BERTæ¨¡å‹ç‰¹æ€§ï¼Œé€šå¸¸ä¼šå‘ç°ï¼š

1. **aten::addmm / aten::mm** (40-50%æ—¶é—´)
   - Linearå±‚çš„çŸ©é˜µä¹˜æ³•
   - CUDAæ–‡ä»¶: `aten/src/ATen/native/cuda/Blas.cpp`
   - è°ƒç”¨cuBLASåº“

2. **aten::softmax** (10-15%æ—¶é—´)
   - Attentionæƒé‡è®¡ç®—
   - CUDAæ–‡ä»¶: `aten/src/ATen/native/cuda/SoftMax.cu`
   - Warp-level reduction

3. **aten::layer_norm** (5-10%æ—¶é—´)
   - Layer Normalization
   - CUDAæ–‡ä»¶: `aten/src/ATen/native/cuda/layer_norm_kernel.cu`
   - Welfordç®—æ³•

4. **aten::gelu** (3-5%æ—¶é—´)
   - FFNæ¿€æ´»å‡½æ•°
   - CUDAæ–‡ä»¶: `aten/src/ATen/native/cuda/Activation.cu`

5. **aten::bmm** (10-15%æ—¶é—´)
   - Attentionä¸­çš„QK^Tå’Œscore*V
   - CUDAæ–‡ä»¶: `aten/src/ATen/native/cuda/Blas.cpp`

## ğŸ“ ç”¨äºå¤§ä½œä¸šçš„è°ƒç ”æµç¨‹

### Step 1: æ”¶é›†æ•°æ® âœ…
```bash
./run_comprehensive_profiling.sh
```

### Step 2: è¯†åˆ«Top 3ç®—å­
```bash
# æŸ¥çœ‹ç”Ÿæˆçš„MarkdownæŠ¥å‘Š
cat profiling_results/*/kernel_analysis_report_*.md | grep "### 4\.[1-3]"
```

### Step 3: æ·±å…¥åˆ†æCUDAå®ç°

å¯¹äºæ¯ä¸ªç®—å­ï¼Œåˆ†æä»¥ä¸‹å†…å®¹ï¼š

#### 3.1 ä¸ºä½•å¯ä»¥å¹¶è¡Œå®ç°
- æ•°æ®ç‹¬ç«‹æ€§åˆ†æ
- å¹¶è¡Œè®¡ç®—çš„ç»´åº¦

#### 3.2 å¹¶è¡Œç»´åº¦çš„é€‰æ‹©
- Block/Threadçš„ç»„ç»‡æ–¹å¼
- Shared memoryä½¿ç”¨ç­–ç•¥
- å¯„å­˜å™¨åˆ†é…

#### 3.3 CUDA Kernelä»£ç é€»è¾‘
- ä¸»è¦è®¡ç®—æµç¨‹
- å†…å­˜è®¿é—®æ¨¡å¼
- åŒæ­¥ç‚¹åˆ†æ

#### 3.4 æ½œåœ¨ä¼˜åŒ–ç©ºé—´
- Memory coalescing
- Bank conflictsé¿å…
- Warp divergenceå‡å°‘
- ç®—å­èåˆæœºä¼š

## ğŸ“š æºç ä½ç½®

PyTorch CUDAç®—å­æºç ï¼š
```
pytorch/aten/src/ATen/native/
â”œâ”€â”€ native_functions.yaml          # ç®—å­å£°æ˜
â”œâ”€â”€ cuda/
â”‚   â”œâ”€â”€ SoftMax.cu                # Softmaxå®ç°
â”‚   â”œâ”€â”€ layer_norm_kernel.cu      # LayerNormå®ç°
â”‚   â”œâ”€â”€ Activation.cu             # æ¿€æ´»å‡½æ•°
â”‚   â”œâ”€â”€ Blas.cpp                  # çŸ©é˜µè¿ç®—ï¼ˆè°ƒç”¨cuBLASï¼‰
â”‚   â””â”€â”€ ...
```

åœ¨çº¿æŸ¥çœ‹ï¼šhttps://github.com/pytorch/pytorch/tree/main/aten/src/ATen/native/cuda

## ğŸ› ï¸ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰é…ç½®
```python
# ä¿®æ”¹ profile_bert.py ä¸­çš„å‚æ•°
parser.add_argument('--batch-sizes', type=int, nargs='+', default=[1, 8, 16])
parser.add_argument('--seq-lens', type=int, nargs='+', default=[128, 256])
```

### ä½¿ç”¨çœŸå®BERTæ¨¡å‹
```bash
pip install transformers
python3 profile_bert.py --use-real-bert
```

### è·³è¿‡benchmark
```bash
python3 profile_bert.py --skip-benchmark
```

## â“ å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•æ‰¾åˆ°ç®—å­å¯¹åº”çš„native_functions.yamlæ¡ç›®ï¼Ÿ

æŸ¥çœ‹ç”Ÿæˆçš„JSONæ–‡ä»¶ä¸­çš„ `native_function` å­—æ®µï¼Œç„¶ååœ¨ `native_functions.yaml` ä¸­æœç´¢ã€‚

### Q2: CUDA kernelåç§°å¤ªé•¿çœ‹ä¸æ¸…ï¼Ÿ

æŸ¥çœ‹JSONæ–‡ä»¶æˆ–ä½¿ç”¨Chrome traceå¯è§†åŒ–ï¼Œå¯ä»¥çœ‹åˆ°å®Œæ•´åç§°ã€‚

### Q3: å¦‚ä½•å¯¹æ¯”ä¸åŒé…ç½®çš„æ€§èƒ½ï¼Ÿ

```python
import json
configs = ['bs1_seq128', 'bs8_seq128', 'bs16_seq128']
for cfg in configs:
    with open(f'profiling_results/full/profiling_stats_{cfg}.json') as f:
        data = json.load(f)
        print(f"{cfg}: {data['summary']['total_cuda_time_ms']:.2f}ms")
```

## ğŸ“ è¿›ä¸€æ­¥å¸®åŠ©

- PyTorch Profileræ–‡æ¡£: https://pytorch.org/docs/stable/profiler.html
- CUDAç¼–ç¨‹æŒ‡å—: https://docs.nvidia.com/cuda/cuda-c-programming-guide/
- Nsight Compute: æ›´è¯¦ç»†çš„kernelçº§åˆ†æå·¥å…·

---

**æœ€åæ›´æ–°**: 2025-12-02

