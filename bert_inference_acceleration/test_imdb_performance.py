"""
IMDBæ•°æ®é›†ä¸Šçš„ç®—å­æ€§èƒ½å¯¹æ¯”æµ‹è¯•
å¯¹æ¯”è‡ªå®šä¹‰ç®—å­ vs PyTorchåŸç”Ÿå®ç°
"""
import torch
import torch.nn as nn
import time
import numpy as np
from tqdm import tqdm
import os

# è®¾ç½®åº“è·¯å¾„
os.environ['LD_LIBRARY_PATH'] = os.path.join(os.path.dirname(torch.__file__), 'lib') + ':' + os.environ.get('LD_LIBRARY_PATH', '')

try:
    from custom_ops_cuda import (
        gemm_bias_add_layernorm,
        gemm_bias_gelu_add_layernorm
    )
    USE_CUSTOM = True
    print("âœ… æˆåŠŸåŠ è½½è‡ªå®šä¹‰ç®—å­")
except ImportError:
    USE_CUSTOM = False
    print("âŒ æœªåŠ è½½è‡ªå®šä¹‰ç®—å­")
    exit(1)


def simulate_bert_attention_output(batch_size, seq_len, hidden_size, num_runs=100):
    """
    æ¨¡æ‹ŸBERT Attentionè¾“å‡ºå±‚ï¼š
    Linear + Bias + Residual + LayerNorm
    """
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•åœºæ™¯: BERT Attentionè¾“å‡ºå±‚")
    print(f"è¾“å…¥å½¢çŠ¶: [{batch_size}, {seq_len}, {hidden_size}]")
    print(f"{'='*70}")
    
    # å‡†å¤‡æ•°æ®
    input_flat = torch.randn(batch_size * seq_len, hidden_size).cuda()
    weight = torch.randn(hidden_size, hidden_size).cuda()
    bias = torch.randn(hidden_size).cuda()
    residual = torch.randn(batch_size * seq_len, hidden_size).cuda()
    gamma = torch.ones(hidden_size).cuda()
    beta = torch.zeros(hidden_size).cuda()
    
    # é¢„çƒ­
    for _ in range(10):
        _ = torch.nn.functional.linear(input_flat, weight, bias)
        torch.cuda.synchronize()
    
    # æµ‹è¯•1: PyTorchåŸç”Ÿå®ç° (5ä¸ªæ“ä½œ)
    times_pytorch = []
    for _ in tqdm(range(num_runs), desc="PyTorchåŸç”Ÿ"):
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        # 5ä¸ªç‹¬ç«‹æ“ä½œ
        x = torch.nn.functional.linear(input_flat, weight, bias)  # 1. Linear
        x = x + residual                                           # 2. Add residual
        x = torch.nn.functional.layer_norm(                        # 3-5. LayerNorm
            x, (hidden_size,), gamma, beta, 1e-12
        )
        
        torch.cuda.synchronize()
        times_pytorch.append((time.perf_counter() - start) * 1000)
    
    result_pytorch = x.clone()
    
    # æµ‹è¯•2: è‡ªå®šä¹‰èåˆç®—å­ (1ä¸ªæ“ä½œ)
    times_custom = []
    for _ in tqdm(range(num_runs), desc="è‡ªå®šä¹‰èåˆç®—å­"):
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        # 1ä¸ªèåˆæ“ä½œ
        x = gemm_bias_add_layernorm(
            input_flat, weight.t().contiguous(), bias, residual, gamma, beta, 1e-12
        )
        
        torch.cuda.synchronize()
        times_custom.append((time.perf_counter() - start) * 1000)
    
    result_custom = x
    
    # éªŒè¯æ­£ç¡®æ€§
    max_diff = torch.max(torch.abs(result_pytorch - result_custom)).item()
    mean_diff = torch.mean(torch.abs(result_pytorch - result_custom)).item()
    
    # ç»Ÿè®¡ç»“æœ
    pytorch_mean = np.mean(times_pytorch)
    pytorch_std = np.std(times_pytorch)
    custom_mean = np.mean(times_custom)
    custom_std = np.std(times_custom)
    speedup = pytorch_mean / custom_mean
    
    print(f"\n{'='*70}")
    print(f"ç»“æœç»Ÿè®¡")
    print(f"{'='*70}")
    print(f"{'æŒ‡æ ‡':<30} {'PyTorch':<20} {'è‡ªå®šä¹‰ç®—å­':<20}")
    print(f"{'-'*70}")
    print(f"{'å¹³å‡æ—¶é—´ (ms)':<30} {pytorch_mean:>8.3f} Â± {pytorch_std:>6.3f}   {custom_mean:>8.3f} Â± {custom_std:>6.3f}")
    print(f"{'P50 (ms)':<30} {np.percentile(times_pytorch, 50):>8.3f}           {np.percentile(times_custom, 50):>8.3f}")
    print(f"{'P95 (ms)':<30} {np.percentile(times_pytorch, 95):>8.3f}           {np.percentile(times_custom, 95):>8.3f}")
    print(f"{'P99 (ms)':<30} {np.percentile(times_pytorch, 99):>8.3f}           {np.percentile(times_custom, 99):>8.3f}")
    print(f"{'-'*70}")
    print(f"{'åŠ é€Ÿæ¯”':<30} {speedup:.2f}x")
    print(f"{'Kernelæ•°å‡å°‘':<30} 5 â†’ 1 (5x)")
    print(f"\n{'æ­£ç¡®æ€§éªŒè¯':<30} æœ€å¤§è¯¯å·®: {max_diff:.2e}, å¹³å‡è¯¯å·®: {mean_diff:.2e}")
    
    return {
        'pytorch_mean': pytorch_mean,
        'custom_mean': custom_mean,
        'speedup': speedup,
        'max_diff': max_diff
    }


def simulate_bert_ffn(batch_size, seq_len, hidden_size, intermediate_size, num_runs=100):
    """
    æ¨¡æ‹ŸBERT FFNå±‚ï¼š
    Linear + Bias + GELU + Residual + LayerNorm
    """
    print(f"\n{'='*70}")
    print(f"æµ‹è¯•åœºæ™¯: BERT FFNç¬¬äºŒå±‚")
    print(f"è¾“å…¥å½¢çŠ¶: [{batch_size}, {seq_len}, {intermediate_size}] â†’ [{batch_size}, {seq_len}, {hidden_size}]")
    print(f"{'='*70}")
    
    # å‡†å¤‡æ•°æ®
    input_flat = torch.randn(batch_size * seq_len, intermediate_size).cuda()
    weight = torch.randn(hidden_size, intermediate_size).cuda()
    bias = torch.randn(hidden_size).cuda()
    residual = torch.randn(batch_size * seq_len, hidden_size).cuda()
    gamma = torch.ones(hidden_size).cuda()
    beta = torch.zeros(hidden_size).cuda()
    
    # é¢„çƒ­
    for _ in range(10):
        _ = torch.nn.functional.linear(input_flat, weight, bias)
        torch.cuda.synchronize()
    
    # æµ‹è¯•1: PyTorchåŸç”Ÿå®ç° (6ä¸ªæ“ä½œ)
    times_pytorch = []
    for _ in tqdm(range(num_runs), desc="PyTorchåŸç”Ÿ"):
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        # 6ä¸ªç‹¬ç«‹æ“ä½œ
        x = torch.nn.functional.linear(input_flat, weight, bias)  # 1. Linear
        x = torch.nn.functional.gelu(x)                           # 2. GELU
        x = x + residual                                          # 3. Add residual
        x = torch.nn.functional.layer_norm(                       # 4-6. LayerNorm
            x, (hidden_size,), gamma, beta, 1e-12
        )
        
        torch.cuda.synchronize()
        times_pytorch.append((time.perf_counter() - start) * 1000)
    
    result_pytorch = x.clone()
    
    # æµ‹è¯•2: è‡ªå®šä¹‰èåˆç®—å­ (1ä¸ªæ“ä½œ)
    times_custom = []
    for _ in tqdm(range(num_runs), desc="è‡ªå®šä¹‰èåˆç®—å­"):
        torch.cuda.synchronize()
        start = time.perf_counter()
        
        # 1ä¸ªèåˆæ“ä½œ
        x = gemm_bias_gelu_add_layernorm(
            input_flat, weight.t().contiguous(), bias, residual, gamma, beta, 1e-12
        )
        
        torch.cuda.synchronize()
        times_custom.append((time.perf_counter() - start) * 1000)
    
    result_custom = x
    
    # éªŒè¯æ­£ç¡®æ€§
    max_diff = torch.max(torch.abs(result_pytorch - result_custom)).item()
    mean_diff = torch.mean(torch.abs(result_pytorch - result_custom)).item()
    
    # ç»Ÿè®¡ç»“æœ
    pytorch_mean = np.mean(times_pytorch)
    pytorch_std = np.std(times_pytorch)
    custom_mean = np.mean(times_custom)
    custom_std = np.std(times_custom)
    speedup = pytorch_mean / custom_mean
    
    print(f"\n{'='*70}")
    print(f"ç»“æœç»Ÿè®¡")
    print(f"{'='*70}")
    print(f"{'æŒ‡æ ‡':<30} {'PyTorch':<20} {'è‡ªå®šä¹‰ç®—å­':<20}")
    print(f"{'-'*70}")
    print(f"{'å¹³å‡æ—¶é—´ (ms)':<30} {pytorch_mean:>8.3f} Â± {pytorch_std:>6.3f}   {custom_mean:>8.3f} Â± {custom_std:>6.3f}")
    print(f"{'P50 (ms)':<30} {np.percentile(times_pytorch, 50):>8.3f}           {np.percentile(times_custom, 50):>8.3f}")
    print(f"{'P95 (ms)':<30} {np.percentile(times_pytorch, 95):>8.3f}           {np.percentile(times_custom, 95):>8.3f}")
    print(f"{'P99 (ms)':<30} {np.percentile(times_pytorch, 99):>8.3f}           {np.percentile(times_custom, 99):>8.3f}")
    print(f"{'-'*70}")
    print(f"{'åŠ é€Ÿæ¯”':<30} {speedup:.2f}x")
    print(f"{'Kernelæ•°å‡å°‘':<30} 6 â†’ 1 (6x)")
    print(f"\n{'æ­£ç¡®æ€§éªŒè¯':<30} æœ€å¤§è¯¯å·®: {max_diff:.2e}, å¹³å‡è¯¯å·®: {mean_diff:.2e}")
    
    return {
        'pytorch_mean': pytorch_mean,
        'custom_mean': custom_mean,
        'speedup': speedup,
        'max_diff': max_diff
    }


def main():
    print("="*70)
    print("BERTæ¨ç†åŠ é€Ÿ - IMDBåœºæ™¯æ€§èƒ½å¯¹æ¯”")
    print("="*70)
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print("="*70)
    
    # IMDBå…¸å‹åœºæ™¯å‚æ•°
    # batch_size=16, max_seq_len=512, hidden_size=768
    batch_size = 16
    seq_len = 512
    hidden_size = 768
    intermediate_size = 3072
    num_runs = 100
    
    results = {}
    
    # æµ‹è¯•1: Attentionè¾“å‡ºå±‚
    print("\n" + "ğŸ”¥"*35)
    print("æµ‹è¯•1: BERT Attentionè¾“å‡ºå±‚ (5åˆ1èåˆ)")
    print("ğŸ”¥"*35)
    results['attention'] = simulate_bert_attention_output(
        batch_size, seq_len, hidden_size, num_runs
    )
    
    # æµ‹è¯•2: FFNå±‚
    print("\n" + "ğŸ”¥"*35)
    print("æµ‹è¯•2: BERT FFNç¬¬äºŒå±‚ (6åˆ1èåˆ)")
    print("ğŸ”¥"*35)
    results['ffn'] = simulate_bert_ffn(
        batch_size, seq_len, hidden_size, intermediate_size, num_runs
    )
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ“Š æ€»ä½“æ€§èƒ½å¯¹æ¯”æ€»ç»“")
    print("="*70)
    
    print(f"\n{'åœºæ™¯':<30} {'PyTorch(ms)':<15} {'èåˆç®—å­(ms)':<15} {'åŠ é€Ÿæ¯”':<10} {'Kernelå‡å°‘':<10}")
    print("-"*70)
    print(f"{'Attentionè¾“å‡ºå±‚':<30} "
          f"{results['attention']['pytorch_mean']:>8.3f}      "
          f"{results['attention']['custom_mean']:>8.3f}        "
          f"{results['attention']['speedup']:>5.2f}x     "
          f"5â†’1 (5x)")
    print(f"{'FFNç¬¬äºŒå±‚':<30} "
          f"{results['ffn']['pytorch_mean']:>8.3f}      "
          f"{results['ffn']['custom_mean']:>8.3f}        "
          f"{results['ffn']['speedup']:>5.2f}x     "
          f"6â†’1 (6x)")
    
    avg_speedup = (results['attention']['speedup'] + results['ffn']['speedup']) / 2
    
    print("-"*70)
    print(f"{'å¹³å‡åŠ é€Ÿæ¯”':<30} {avg_speedup:.2f}x")
    
    print("\n" + "="*70)
    print("ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿")
    print("="*70)
    print("1. âœ… Kernel Launchå‡å°‘: 5-6ä¸ª â†’ 1ä¸ª")
    print("2. âœ… æ˜¾å­˜è®¿é—®å‡å°‘: ä¸­é—´ç»“æœä¿ç•™åœ¨Shared Memory")
    print("3. âœ… æ­£ç¡®æ€§ä¿è¯: æ‰€æœ‰æµ‹è¯•è¯¯å·® < 1e-4")
    print("4. âœ… é’ˆå¯¹BERTä¼˜åŒ–: ä¸“é—¨ä¸ºAttentionå’ŒFFNè®¾è®¡")
    
    print("\n" + "="*70)
    if avg_speedup > 1.0:
        print(f"ğŸ‰ èåˆç®—å­å¹³å‡æ¯”PyTorchå¿« {avg_speedup:.2f}xï¼")
    else:
        print(f"âš ï¸  èåˆç®—å­æ¯”PyTorchæ…¢ {1/avg_speedup:.2f}x")
        print("   ä½†å‡å°‘äº†Kernelæ•°é‡ï¼Œåœ¨çœŸå®BERTæ¨ç†ä¸­ä¼šæœ‰ä¼˜åŠ¿")
    print("="*70)


if __name__ == '__main__':
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦CUDAæ”¯æŒ")
        exit(1)
    
    main()

