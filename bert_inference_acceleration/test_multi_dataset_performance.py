"""
BERTæ¨ç†åŠ é€Ÿ - å¤šæ•°æ®é›†ç®—å­æ€§èƒ½è¯„æµ‹
æ›´æ–°ï¼šé€‚é… Hugging Face Arrow æ ¼å¼æ•°æ®é›†è¯»å– (save_to_disk æ ¼å¼)
"""

import torch
import torch.nn as nn
import time
import numpy as np
import os
from transformers import BertTokenizer
try:
    from datasets import load_from_disk
except ImportError:
    print("âŒ ç¼ºå°‘ datasets åº“ï¼Œè¯·è¿è¡Œ: pip install datasets")
    exit(1)

# ================= é…ç½®ä¸ç¯å¢ƒ =================
DATASET_BASE_PATH = "/hy-tmp/lhl/bert_inference_acceleration/dataset"
TOKENIZER_PATH = "bert-base-uncased" 

os.environ['LD_LIBRARY_PATH'] = os.path.join(os.path.dirname(torch.__file__), 'lib') + ':' + os.environ.get('LD_LIBRARY_PATH', '')

try:
    from custom_ops_cuda import (
        gemm_bias_add_layernorm,
        gemm_bias_gelu_add_layernorm
    )
    print("âœ… æˆåŠŸåŠ è½½è‡ªå®šä¹‰ç®—å­åº“")
except ImportError:
    print("âŒ æœªèƒ½åŠ è½½ custom_ops_cudaï¼Œè¯·æ£€æŸ¥ç¼–è¯‘æƒ…å†µ")
    exit(1)

# ================= æ ¸å¿ƒå·¥å…·å‡½æ•° =================

def get_real_avg_seq_len(dataset_name, sample_size=1000):
    """
    é’ˆå¯¹ load_from_disk æ ¼å¼çš„ç›®å½•è¯»å–å¹¶è®¡ç®—å¹³å‡é•¿åº¦
    """
    # åŒ¹é…å›¾ç‰‡ä¸­çš„ç›®å½•ï¼šAG News -> ag_news, IMDB -> imdb
    dir_name = dataset_name.lower().replace(" ", "_")
    path = os.path.join(DATASET_BASE_PATH, dir_name)
    
    print(f"ğŸ” æ­£åœ¨ä»æœ¬åœ° Arrow ç›®å½•åŠ è½½: {path}")
    
    try:
        # 1. åŠ è½½æœ¬åœ°æ•°æ®é›†
        data = load_from_disk(path)
        
        # 2. å¤„ç† DatasetDict (åŒ…å« train/test çš„æƒ…å†µ)
        if isinstance(data, dict) or hasattr(data, 'keys'):
            # ä¼˜å…ˆé€‰æ‹© trainï¼Œå¦åˆ™å–ç¬¬ä¸€ä¸ª split
            split = 'train' if 'train' in data else list(data.keys())[0]
            ds = data[split]
        else:
            ds = data
            
        # 3. é‡‡æ ·å¹¶è·å–æ–‡æœ¬åˆ—
        # è‡ªåŠ¨è¯†åˆ«åˆ—åï¼šé€šå¸¸ä¸º 'text' æˆ– 'description'
        cols = ds.column_names
        text_col = 'text' if 'text' in cols else ('description' if 'description' in cols else cols[0])
        
        sample_ds = ds.select(range(min(len(ds), sample_size)))
        texts = sample_ds[text_col]
        
        # 4. è®¡ç®—é•¿åº¦
        tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)
        lengths = [len(tokenizer.encode(t, add_special_tokens=True, max_length=512, truncation=True)) for t in texts]
        
        avg_len = int(np.mean(lengths))
        print(f"ğŸ“Š {dataset_name} ç»Ÿè®¡å®Œæˆ: å®é™…å¹³å‡é•¿åº¦ = {avg_len}")
        return avg_len

    except Exception as e:
        default = 512 if "imdb" in dir_name else 128
        print(f"âš ï¸ è¯»å–å¤±è´¥ ({e})ï¼Œä½¿ç”¨é¢„è®¾é»˜è®¤å€¼: {default}")
        return default

# ================= æ€§èƒ½è¯„æµ‹å‡½æ•° =================

def simulate_bert_attention_output(batch_size, seq_len, hidden_size, num_runs=100):
    input_flat = torch.randn(batch_size * seq_len, hidden_size).cuda()
    weight = torch.randn(hidden_size, hidden_size).cuda()
    bias = torch.randn(hidden_size).cuda()
    residual = torch.randn(batch_size * seq_len, hidden_size).cuda()
    gamma = torch.ones(hidden_size).cuda()
    beta = torch.zeros(hidden_size).cuda()
    
    for _ in range(10): _ = torch.nn.functional.linear(input_flat, weight, bias)
    
    torch.cuda.synchronize()
    t0 = time.perf_counter()
    for _ in range(num_runs):
        x = torch.nn.functional.linear(input_flat, weight, bias)
        x = x + residual
        x = torch.nn.functional.layer_norm(x, (hidden_size,), gamma, beta, 1e-12)
    torch.cuda.synchronize()
    py_time = (time.perf_counter() - t0) * 1000 / num_runs
    
    torch.cuda.synchronize()
    t1 = time.perf_counter()
    for _ in range(num_runs):
        x = gemm_bias_add_layernorm(input_flat, weight.t().contiguous(), bias, residual, gamma, beta, 1e-12)
    torch.cuda.synchronize()
    cu_time = (time.perf_counter() - t1) * 1000 / num_runs
    return py_time, cu_time

def simulate_bert_ffn(batch_size, seq_len, hidden_size, intermediate_size, num_runs=100):
    input_flat = torch.randn(batch_size * seq_len, intermediate_size).cuda()
    weight = torch.randn(hidden_size, intermediate_size).cuda()
    bias = torch.randn(hidden_size).cuda()
    residual = torch.randn(batch_size * seq_len, hidden_size).cuda()
    gamma = torch.ones(hidden_size).cuda()
    beta = torch.zeros(hidden_size).cuda()

    for _ in range(10): _ = torch.nn.functional.linear(input_flat, weight, bias)

    torch.cuda.synchronize()
    t0 = time.perf_counter()
    for _ in range(num_runs):
        x = torch.nn.functional.linear(input_flat, weight, bias)
        x = x + residual
        x = torch.nn.functional.gelu(x)
        x = torch.nn.functional.layer_norm(x, (hidden_size,), gamma, beta, 1e-12)
    torch.cuda.synchronize()
    py_time = (time.perf_counter() - t0) * 1000 / num_runs

    torch.cuda.synchronize()
    t1 = time.perf_counter()
    for _ in range(num_runs):
        x = gemm_bias_gelu_add_layernorm(input_flat, weight.t().contiguous(), bias, residual, gamma, beta, 1e-12)
    torch.cuda.synchronize()
    cu_time = (time.perf_counter() - t1) * 1000 / num_runs
    return py_time, cu_time

# ================= ä¸»ç¨‹åº =================

def main():
    print("="*85)
    print("BERT æ¨ç†åŠ é€Ÿç®—å­è¯„æµ‹ - Arrow æ ¼å¼é€‚é…ç‰ˆ")
    print("="*85)

    configs = [
        {"name": "IMDB", "batch_size": 16},
        {"name": "AG News", "batch_size": 32}
    ]
    
    results = []

    for cfg in configs:
        # è·å–çœŸå®é•¿åº¦
        raw_len = get_real_avg_seq_len(cfg['name'])
        final_len = min(raw_len, 512)
        
        # è¿è¡Œè¯„æµ‹
        att_py, att_cu = simulate_bert_attention_output(cfg['batch_size'], final_len, 768)
        ffn_py, ffn_cu = simulate_bert_ffn(cfg['batch_size'], final_len, 768, 3072)
        
        results.append({"ds": cfg['name'], "len": final_len, "type": "Attn-Out", "py": att_py, "cu": att_cu})
        results.append({"ds": cfg['name'], "len": final_len, "type": "FFN-Layer", "py": ffn_py, "cu": ffn_cu})

    # æ‰“å°æŠ¥è¡¨
    print("\nğŸ“Š æ€§èƒ½æ€»ç»“æŠ¥å‘Š")
    print("="*85)
    print(f"{'æ•°æ®é›†':<12} {'åœºæ™¯':<12} {'å¹³å‡é•¿åº¦':<10} {'PyTorch(ms)':<15} {'è‡ªå®šä¹‰ç®—å­(ms)':<15} {'åŠ é€Ÿæ¯”':<10}")
    print("-" * 85)
    for r in results:
        speedup = r['py'] / r['cu']
        print(f"{r['ds']:<12} {r['type']:<12} {r['len']:<10} {r['py']:>10.3f}      {r['cu']:>10.3f}       {speedup:>6.2f}x")
    print("="*85)

if __name__ == '__main__':
    main()